{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00ec82f0",
   "metadata": {},
   "source": [
    "## MS MACRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5a1ba46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wwon0076\\Desktop\\FYP\\experiment_metric\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from sentence_transformers import CrossEncoder\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "108ea916",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cd47c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526cafb2",
   "metadata": {},
   "source": [
    "Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82494d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Ground Truth & Candidate Pool\n",
    "BASE = \"ms_macro\"\n",
    "QRELS_FILE = f\"{BASE}/qrels.dev.tsv\"\n",
    "CANDS_FILE = f\"{BASE}/top1000.dev\"   # BM25 top-1000 pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b03728b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Ground Truth [question id, unused, passage id, ground truth]\n",
    "qrels = pd.read_csv(\n",
    "    QRELS_FILE,\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"qid\", \"unused\", \"pid\", \"rel\"]\n",
    ")[[\"qid\", \"pid\", \"rel\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4e3b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BM25 candidates [question id passage id, query, passage]\n",
    "top1000 = pd.read_csv(\n",
    "    CANDS_FILE,\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"qid\", \"pid\", \"query\", \"passage\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2537c5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Shape of each datasets =====\n",
      "qrels: (59273, 3) | unique qids: 55578\n",
      "top1000: (6668967, 4) | unique qids: 6980\n",
      "\n",
      "===== Ground Truth where it says 'relevant' =====\n",
      "rel\n",
      "1    59273\n",
      "Name: count, dtype: int64\n",
      "\n",
      "===== Ground Truth Datasets Looking =====\n",
      "       qid      pid  rel\n",
      "0  1102432  2026790    1\n",
      "1  1102431  7066866    1\n",
      "2  1102431  7066867    1\n",
      "3  1090282  7066900    1\n",
      "4    39449  7066905    1\n",
      "\n",
      "===== BM25 Candidate Datasets Looking =====\n",
      "        qid      pid                                              query  \\\n",
      "0    188714  1000052         foods and supplements to lower blood sugar   \n",
      "1   1082792  1000084  what does the golgi apparatus do to the protei...   \n",
      "2    995526  1000094           where is the federal penitentiary in ind   \n",
      "3    199776  1000115               health benefits of eating vegetarian   \n",
      "4    660957  1000115              what foods are good if you have gout?   \n",
      "..      ...      ...                                                ...   \n",
      "95   684459  1001292               what is a gel injection in your knee   \n",
      "96   591940  1001292                    what causes spasms in your knee   \n",
      "97   313940   100132              how much does general anesthesia cost   \n",
      "98  1043545  1001336                    what is the buffer stock model?   \n",
      "99   510893  1001378                                 tceq renewal hours   \n",
      "\n",
      "                                              passage  \n",
      "0   Watch portion sizes: ■ Even healthy foods will...  \n",
      "1   Start studying Bonding, Carbs, Proteins, Lipid...  \n",
      "2   It takes THOUSANDS of Macy's associates to bri...  \n",
      "3   The good news is that you will discover what g...  \n",
      "4   The good news is that you will discover what g...  \n",
      "..                                                ...  \n",
      "95  Knee swelling in children. Knee swelling in ch...  \n",
      "96  Knee swelling in children. Knee swelling in ch...  \n",
      "97  Knee replacement surgery usually takes 1 to 2 ...  \n",
      "98  There is an all-up PLE counter as well as indi...  \n",
      "99  How Long Does A Renewal Passport Take? Passpor...  \n",
      "\n",
      "[100 rows x 4 columns]\n",
      "\n",
      "===== Making Sure BM25 has actually 1000 passage for each query =====\n",
      "Number of rows for Query 188714: 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"===== Shape of each datasets =====\")\n",
    "print(\"qrels:\", qrels.shape, \"| unique qids:\", qrels[\"qid\"].nunique())\n",
    "print(\"top1000:\", top1000.shape, \"| unique qids:\", top1000[\"qid\"].nunique())\n",
    "\n",
    "print(\"\\n===== Ground Truth where it says 'relevant' =====\")\n",
    "print(qrels[\"rel\"].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "print(\"\\n===== Ground Truth Datasets Looking =====\")\n",
    "print(qrels.head())\n",
    "\n",
    "\n",
    "print(\"\\n===== BM25 Candidate Datasets Looking =====\")\n",
    "print(top1000.head(100))\n",
    "\n",
    "print(\"\\n===== Making Sure BM25 has actually 1000 passage for each query =====\")\n",
    "count = top1000[top1000['qid'] == 1082792].shape[0] # 12 is random qid here\n",
    "print(f\"Number of rows for Query 188714: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460677ed",
   "metadata": {},
   "source": [
    "Combine them (all in variable df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19a2a01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check the Answer Key\n",
    "# adding new colomn 'rel' into df of the top 1000 candidate datasets\n",
    "# the qid and pid from the qrels are combined into the qid and pid from top 1000 candidate datasets\n",
    "df = top1000.merge(qrels, on=[\"qid\", \"pid\"], how=\"left\") # [\"qid\", \"pid\", \"query\", \"passage\", \"rel\"]\n",
    "\n",
    "# 2. Mark wrong answers as 0\n",
    "# The answer key only contains the Correct (1) answers.\n",
    "# So, we fill all the empty spots (NaN) with 0.\n",
    "df[\"rel\"] = df[\"rel\"].fillna(0).astype(int) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83335b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total candidates: 6668967\n",
      "Distinct queries: 6980\n",
      "Relevant pairs in candidates: 6005\n"
     ]
    }
   ],
   "source": [
    "print(\"Total candidates:\", len(df))\n",
    "print(\"Distinct queries:\", df[\"qid\"].nunique())\n",
    "print(\"Relevant pairs in candidates:\", int((df[\"rel\"] == 1).sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a940cf08",
   "metadata": {},
   "source": [
    "Create 100 Query \n",
    "Conditions: \n",
    "1. Find query that has exactly 1 relevance mark from ground truth, of total number of Q_LIMIT (no. query, select 100 default)\n",
    "2. Total passage (relevant + non-relevant) must sum to maximum of K = 1000\n",
    "3. skip that query it has no these conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fb1995b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5161 queries that match your criteria.\n",
      "\n",
      "===== Evaluation Set Created =====\n",
      "Total Rows: 100000\n",
      "Unique Queries: 100\n",
      "Total Relevant: 100\n",
      "       qid      pid                                         query  \\\n",
      "16  995825  1000492  where is the graphic card located in the cpu   \n",
      "17  995825  1000494  where is the graphic card located in the cpu   \n",
      "85  480064  1001246                 price chopper locations in ct   \n",
      "88  480064  1001252                 price chopper locations in ct   \n",
      "91  480064  1001253                 price chopper locations in ct   \n",
      "\n",
      "                                              passage  rel  \n",
      "16  For example, a “PC Expansion Card” maybe the j...    0  \n",
      "17  The Common Cards & Buses. The most common type...    0  \n",
      "85  When I want a T-bone steak or chuck steak for ...    0  \n",
      "88  Ew...Price Chopper has gone downhill. I shoppe...    0  \n",
      "91  I love going to Price Chopper because I can fi...    0  \n"
     ]
    }
   ],
   "source": [
    "# 1. Calculate stats for every query\n",
    "# We count how many relevant items (sum) and how many total candidates (count) each QID has.\n",
    "q_stats = df.groupby(\"qid\").agg(\n",
    "    relevant_count=(\"rel\", \"sum\"),\n",
    "    total_candidates=(\"pid\", \"count\")\n",
    ")\n",
    "\n",
    "# 2. Filter: Find QIDs that match your strict criteria\n",
    "# Condition A: Exactly 1 relevant answer\n",
    "# Condition B: Exactly 1000 candidates (standard for MS MARCO)\n",
    "valid_qids = q_stats[\n",
    "    (q_stats[\"relevant_count\"] == 1) & \n",
    "    (q_stats[\"total_candidates\"] == 1000)\n",
    "].index.tolist()\n",
    "\n",
    "print(f\"Found {len(valid_qids)} queries that match your criteria.\")\n",
    "\n",
    "# 3. Randomly select 100 QIDs\n",
    "# (We use a seed so you get the same 100 queries every time you run this)\n",
    "random.seed(42)\n",
    "selected_qids = random.sample(valid_qids, 100)\n",
    "\n",
    "# 4. Create the final Evaluation Dataset\n",
    "# We filter the big 'df' to keep only the rows belonging to our 100 chosen queries.\n",
    "eval_set = df[df[\"qid\"].isin(selected_qids)].copy()\n",
    "\n",
    "# 5. Verify the Result\n",
    "print(\"\\n===== Evaluation Set Created =====\")\n",
    "print(\"Total Rows:\", len(eval_set))      # Should be 100,000 (100 queries * 1000 docs)\n",
    "print(\"Unique Queries:\", eval_set[\"qid\"].nunique()) # Should be 100\n",
    "print(\"Total Relevant:\", eval_set[\"rel\"].sum())     # Should be 100 (1 per query)\n",
    "\n",
    "# Show a preview\n",
    "print(eval_set.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f068b9",
   "metadata": {},
   "source": [
    "Experiment Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f6d83c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 105/105 [00:00<00:00, 1588.80it/s, Materializing param=classifier.weight]                                    \n",
      "BertForSequenceClassification LOAD REPORT from: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computing token lengths for sorting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:11<00:00, 8433.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "FEED_SIZE = 32   # Batch size (Try 16 or 32 for Pi, 64 or 128 for HP Z2)\n",
    "MODEL_NAME = 'cross-encoder/ms-marco-MiniLM-L-6-v2'\n",
    "\n",
    "# Load Model\n",
    "model = CrossEncoder(MODEL_NAME, token=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=False)\n",
    "\n",
    "# Prepare Data: Convert DataFrame to a clean list of dictionaries for easier processing\n",
    "# We keep the original index to map scores back later\n",
    "data_records = eval_set.reset_index().to_dict('records')\n",
    "\n",
    "# Pre-compute token lengths for the \"Proposed\" sorting strategy\n",
    "# (We do this once to avoid measuring tokenization overhead in the sorting time)\n",
    "print(\"Pre-computing token lengths for sorting...\")\n",
    "for row in tqdm(data_records):\n",
    "    # We simulate the exact input the model sees: [CLS] query [SEP] passage [SEP]\n",
    "    tokens = tokenizer(row['query'], row['passage'], truncation=True, max_length=512)\n",
    "    row['token_len'] = len(tokens['input_ids'])\n",
    "\n",
    "\n",
    "def run_benchmark(records, method_name, sort_by_length=False):\n",
    "    \"\"\"\n",
    "    Runs the reranker with specific batching logic and tracks latency/padding.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Group by Query (So we can measure Per-Query Latency)\n",
    "    #    We assume the input 'records' is a list of all 100,000 rows.\n",
    "    grouped_data = {}\n",
    "    for r in records:\n",
    "        if r['qid'] not in grouped_data:\n",
    "            grouped_data[r['qid']] = []\n",
    "        grouped_data[r['qid']].append(r)\n",
    "        \n",
    "    query_latencies = []\n",
    "    total_padding_waste = []\n",
    "    total_actual_tokens = []\n",
    "    total_chunks = 0\n",
    "    \n",
    "    # Storage for results: {index: score}\n",
    "    score_map = {}\n",
    "    \n",
    "    start_global = time.time()\n",
    "    \n",
    "    # Process one query at a time (to get min/max/median per query)\n",
    "    for qid, docs in tqdm(grouped_data.items(), desc=f\"Running {method_name}\"):\n",
    "        \n",
    "        t0 = time.time()\n",
    "        \n",
    "        # --- A. SORTING STRATEGY ---\n",
    "        if sort_by_length:\n",
    "            # PROPOSED: Sort docs by length (Shortest to Longest)\n",
    "            # This minimizes padding because similar lengths are batched together\n",
    "            docs.sort(key=lambda x: x['token_len'])\n",
    "        else:\n",
    "            # VANILLA: Random shuffle (simulate typical dataloader)\n",
    "            # We use a fixed seed for reproducibility\n",
    "            np.random.RandomState(42).shuffle(docs)\n",
    "            \n",
    "        # --- B. BATCHING LOOP ---\n",
    "        # Create pairs for the model: [[q, p], [q, p]...]\n",
    "        pairs = [[d['query'], d['passage']] for d in docs]\n",
    "        indices = [d['index'] for d in docs] # Keep track of original ID\n",
    "        \n",
    "        for i in range(0, len(pairs), FEED_SIZE):\n",
    "            batch_pairs = pairs[i : i + FEED_SIZE]\n",
    "            batch_indices = indices[i : i + FEED_SIZE]\n",
    "            \n",
    "            # 1. Measure Padding Waste (Simulation)\n",
    "            # We explicitly tokenize the batch to see what the model WOULD do\n",
    "            encoded = tokenizer(\n",
    "                batch_pairs, \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                max_length=512, \n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Calc stats\n",
    "            batch_max_len = encoded['input_ids'].shape[1]\n",
    "            batch_size_actual = encoded['input_ids'].shape[0]\n",
    "            \n",
    "            # Count non-padding tokens (attention_mask is 1 for real tokens)\n",
    "            actual_toks = torch.sum(encoded['attention_mask']).item()\n",
    "            total_slots = batch_max_len * batch_size_actual\n",
    "            waste = total_slots - actual_toks\n",
    "            \n",
    "            total_actual_tokens.append(actual_toks)\n",
    "            total_padding_waste.append(waste)\n",
    "            total_chunks += 1\n",
    "            \n",
    "            # 2. Actual Inference\n",
    "            # We perform the actual prediction to get the latency\n",
    "            batch_scores = model.predict(batch_pairs, batch_size=FEED_SIZE, show_progress_bar=False)\n",
    "            \n",
    "            # Store scores\n",
    "            for idx, score in zip(batch_indices, batch_scores):\n",
    "                score_map[idx] = score\n",
    "\n",
    "        # Measure Latency for this one query\n",
    "        query_latencies.append((time.time() - t0) * 1000) # ms\n",
    "\n",
    "    total_wall_time = time.time() - start_global\n",
    "    \n",
    "    return {\n",
    "        \"scores\": score_map,\n",
    "        \"latencies\": query_latencies,\n",
    "        \"wall_time\": total_wall_time,\n",
    "        \"chunks\": total_chunks,\n",
    "        \"avg_actual\": np.mean(total_actual_tokens) / FEED_SIZE, # Approx per chunk\n",
    "        \"avg_padded\": (np.sum(total_actual_tokens) + np.sum(total_padding_waste)) / total_chunks / FEED_SIZE, # Approx\n",
    "        \"waste_pct\": (np.sum(total_padding_waste) / (np.sum(total_actual_tokens) + np.sum(total_padding_waste))) * 100\n",
    "    }\n",
    "\n",
    "def calculate_metrics(df, score_col):\n",
    "    \"\"\"\n",
    "    Calculates MRR@10/100 and HitRate@10/100\n",
    "    \"\"\"\n",
    "    mrr10, mrr100, hit10, hit100 = [], [], [], []\n",
    "    \n",
    "    for qid, group in df.groupby(\"qid\"):\n",
    "        # Sort by the Cross Encoder Score (High to Low)\n",
    "        sorted_group = group.sort_values(score_col, ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        # Find where the relevant doc (rel=1) is\n",
    "        # We assume only 1 relevant doc per query for this dataset\n",
    "        relevant_rank = sorted_group.index[sorted_group['rel'] == 1].tolist()\n",
    "        \n",
    "        if not relevant_rank:\n",
    "            # Should not happen in our filtered dataset\n",
    "            mrr10.append(0); mrr100.append(0); hit10.append(0); hit100.append(0)\n",
    "            continue\n",
    "            \n",
    "        rank = relevant_rank[0] + 1 # 1-based index\n",
    "        \n",
    "        # MRR\n",
    "        mrr10.append(1/rank if rank <= 10 else 0)\n",
    "        mrr100.append(1/rank if rank <= 100 else 0)\n",
    "        \n",
    "        # Hit Rate\n",
    "        hit10.append(1 if rank <= 10 else 0)\n",
    "        hit100.append(1 if rank <= 100 else 0)\n",
    "        \n",
    "    return np.mean(mrr10), np.mean(mrr100), np.mean(hit10), np.mean(hit100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537e2012",
   "metadata": {},
   "source": [
    "Vanilla Cross Encoder (Random Order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8679bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running VANILLA:   0%|          | 0/100 [00:00<?, ?it/s]c:\\Users\\wwon0076\\Desktop\\FYP\\experiment_metric\\venv\\lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:92: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Running VANILLA: 100%|██████████| 100/100 [00:59<00:00,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "[VANILLA] Queries: 100 | total pairs: 100,000\n",
      "[VANILLA] Per-query CE latency (feed 32 at a time):\n",
      "  mean = 590.8 ms   median = 558.8 ms   min/max = 418.6/899.3 ms\n",
      "  total wall time = 59.15 s\n",
      "\n",
      "[VANILLA] Token padding waste (per 32-sized chunk):\n",
      "  avg padded length  = 161.4 tokens\n",
      "  avg actual length  = 82.7 tokens\n",
      "  avg padding waste  = 48.8% (lower is better)\n",
      "  chunks processed   = 3,200\n",
      "\n",
      "[VANILLA] === Metrics ===\n",
      "MRR@10      : 0.4336\n",
      "MRR@100     : 0.4431\n",
      "HitRate@10  : 0.7600\n",
      "HitRate@100 : 0.9600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 1. RUN VANILLA (Random Order) ---\n",
    "results_vanilla = run_benchmark(data_records, \"VANILLA\", sort_by_length=False)\n",
    "\n",
    "# Store scores back to dataframe\n",
    "eval_set['score_vanilla'] = eval_set.index.map(results_vanilla['scores'])\n",
    "\n",
    "# Calc Metrics\n",
    "mrr10_v, mrr100_v, hit10_v, hit100_v = calculate_metrics(eval_set, 'score_vanilla')\n",
    "\n",
    "# Print Vanilla Report\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"[VANILLA] Queries: {eval_set['qid'].nunique()} | total pairs: {len(eval_set):,}\")\n",
    "print(f\"[VANILLA] Per-query CE latency (feed {FEED_SIZE} at a time):\")\n",
    "lats = results_vanilla['latencies']\n",
    "print(f\"  mean = {np.mean(lats):.1f} ms   median = {np.median(lats):.1f} ms   min/max = {np.min(lats):.1f}/{np.max(lats):.1f} ms\")\n",
    "print(f\"  total wall time = {results_vanilla['wall_time']:.2f} s\")\n",
    "\n",
    "print(f\"\\n[VANILLA] Token padding waste (per {FEED_SIZE}-sized chunk):\")\n",
    "print(f\"  avg padded length  = {results_vanilla['avg_padded']:.1f} tokens\")\n",
    "print(f\"  avg actual length  = {results_vanilla['avg_actual']:.1f} tokens\")\n",
    "print(f\"  avg padding waste  = {results_vanilla['waste_pct']:.1f}% (lower is better)\")\n",
    "print(f\"  chunks processed   = {results_vanilla['chunks']:,}\")\n",
    "\n",
    "print(\"\\n[VANILLA] === Metrics ===\")\n",
    "print(f\"MRR@10      : {mrr10_v:.4f}\")\n",
    "print(f\"MRR@100     : {mrr100_v:.4f}\")\n",
    "print(f\"HitRate@10  : {hit10_v:.4f}\")\n",
    "print(f\"HitRate@100 : {hit100_v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114de913",
   "metadata": {},
   "source": [
    "Proposed Cross Encoder (Sorted Order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c690892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running PROPOSED: 100%|██████████| 100/100 [00:33<00:00,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "[PROPOSED] Queries: 100 | total pairs: 100,000\n",
      "[PROPOSED] Per-query CE latency (feed 32 at a time):\n",
      "  mean = 330.2 ms   median = 328.4 ms   min/max = 247.5/536.1 ms\n",
      "  total wall time = 33.09 s\n",
      "\n",
      "[PROPOSED] Token padding waste (per 32-sized chunk):\n",
      "  avg padded length  = 85.3 tokens\n",
      "  avg actual length  = 82.7 tokens\n",
      "  avg padding waste  = 3.1% (lower is better)\n",
      "  chunks processed   = 3,200\n",
      "\n",
      "[PROPOSED] === Metrics ===\n",
      "MRR@10      : 0.4336\n",
      "MRR@100     : 0.4431\n",
      "HitRate@10  : 0.7600\n",
      "HitRate@100 : 0.9600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 2. RUN PROPOSED (Sorted Order) ---\n",
    "results_proposed = run_benchmark(data_records, \"PROPOSED\", sort_by_length=True)\n",
    "\n",
    "# Store scores\n",
    "eval_set['score_proposed'] = eval_set.index.map(results_proposed['scores'])\n",
    "\n",
    "# Calc Metrics (Should be identical or very close to Vanilla)\n",
    "mrr10_p, mrr100_p, hit10_p, hit100_p = calculate_metrics(eval_set, 'score_proposed')\n",
    "\n",
    "# Print Proposed Report\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"[PROPOSED] Queries: {eval_set['qid'].nunique()} | total pairs: {len(eval_set):,}\")\n",
    "print(f\"[PROPOSED] Per-query CE latency (feed {FEED_SIZE} at a time):\")\n",
    "lats_p = results_proposed['latencies']\n",
    "print(f\"  mean = {np.mean(lats_p):.1f} ms   median = {np.median(lats_p):.1f} ms   min/max = {np.min(lats_p):.1f}/{np.max(lats_p):.1f} ms\")\n",
    "print(f\"  total wall time = {results_proposed['wall_time']:.2f} s\")\n",
    "\n",
    "print(f\"\\n[PROPOSED] Token padding waste (per {FEED_SIZE}-sized chunk):\")\n",
    "print(f\"  avg padded length  = {results_proposed['avg_padded']:.1f} tokens\")\n",
    "print(f\"  avg actual length  = {results_proposed['avg_actual']:.1f} tokens\")\n",
    "print(f\"  avg padding waste  = {results_proposed['waste_pct']:.1f}% (lower is better)\")\n",
    "print(f\"  chunks processed   = {results_proposed['chunks']:,}\")\n",
    "\n",
    "print(\"\\n[PROPOSED] === Metrics ===\")\n",
    "print(f\"MRR@10      : {mrr10_v:.4f}\")\n",
    "print(f\"MRR@100     : {mrr100_v:.4f}\")\n",
    "print(f\"HitRate@10  : {hit10_v:.4f}\")\n",
    "print(f\"HitRate@100 : {hit100_v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d53b16",
   "metadata": {},
   "source": [
    "## Retail Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38de09c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import CrossEncoder\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- HUGGING FACE TOKEN FIX ---\n",
    "os.environ[\"HF_HUB_DISABLE_IMPLICIT_TOKEN\"] = \"1\" \n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "FEED_SIZE = 32      # Batch size\n",
    "DATASET_PATH = \"retail_qna_eval_100.json\"\n",
    "CHROMA_PATH = \"db\"\n",
    "MODEL_NAME = 'cross-encoder/ms-marco-MiniLM-L-6-v2'\n",
    "\n",
    "# --- LlamaIndex & LangChain Imports ---\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core import VectorStoreIndex, Settings, StorageContext\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "print(\"Libraries loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f442666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 1532.40it/s, Materializing param=pooler.dense.weight]                               \n",
      "BertModel LOAD REPORT from: BAAI/bge-small-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n",
      "Connecting to ChromaDB at db...\n",
      "Building BM25 Index from Chroma documents...\n",
      "Retrievers Ready. Total Documents in DB: 169\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup Embedding Model\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = None\n",
    "\n",
    "# 2. Connect to ChromaDB\n",
    "print(f\"Connecting to ChromaDB at {CHROMA_PATH}...\")\n",
    "db2 = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "chroma_collection = db2.get_or_create_collection(\"retail_qna\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# 3. Create Dense Retriever (LlamaIndex)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context)\n",
    "dense_retriever = VectorIndexRetriever(index=index, similarity_top_k=50)\n",
    "dense_query_engine = RetrieverQueryEngine(retriever=dense_retriever)\n",
    "\n",
    "# 4. Create Sparse Retriever (BM25)\n",
    "# We pull all docs from Chroma to build the BM25 index in memory\n",
    "print(\"Building BM25 Index from Chroma documents...\")\n",
    "all_docs = chroma_collection.get()\n",
    "documents = [Document(page_content=text, metadata={}) for text in all_docs['documents']]\n",
    "bm25_retriever = BM25Retriever.from_documents(documents=documents, k=50)\n",
    "\n",
    "print(f\"Retrievers Ready. Total Documents in DB: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8182b1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 queries from retail_qna_eval_100.json\n",
      "Retrieving candidates for all queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 76.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs to rerank: 7601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Queries\n",
    "with open(DATASET_PATH, 'r') as f:\n",
    "    query_data = json.load(f)\n",
    "\n",
    "# Extract just the text strings\n",
    "queries = [item['text'] for item in query_data]\n",
    "print(f\"Loaded {len(queries)} queries from {DATASET_PATH}\")\n",
    "\n",
    "# 2. Run Retrieval Loop (Hybrid: Dense + Sparse)\n",
    "retrieved_records = []\n",
    "global_idx = 0\n",
    "\n",
    "print(\"Retrieving candidates for all queries...\")\n",
    "for q_idx, query_text in tqdm(enumerate(queries), total=len(queries)):\n",
    "    \n",
    "    # A. Dense Retrieval\n",
    "    dense_results = dense_query_engine.retrieve(query_text)\n",
    "    dense_texts = [n.node.get_text() for n in dense_results]\n",
    "    \n",
    "    # B. Sparse Retrieval (BM25)\n",
    "    sparse_results = bm25_retriever.invoke(query_text) # .invoke is newer than get_relevant_documents\n",
    "    sparse_texts = [doc.page_content for doc in sparse_results]\n",
    "    \n",
    "    # C. Combine & Deduplicate\n",
    "    # We use a set to ensure we don't rerank the same passage twice\n",
    "    unique_candidates = list(set(dense_texts + sparse_texts))\n",
    "    \n",
    "    # D. Store for Reranking\n",
    "    for passage in unique_candidates:\n",
    "        retrieved_records.append({\n",
    "            'index': global_idx,\n",
    "            'qid': q_idx,           # Group by Query ID\n",
    "            'query': query_text,\n",
    "            'passage': passage\n",
    "        })\n",
    "        global_idx += 1\n",
    "\n",
    "print(f\"Total pairs to rerank: {len(retrieved_records)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "037f3f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Cross-Encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 105/105 [00:00<00:00, 1555.34it/s, Materializing param=classifier.weight]                                    \n",
      "BertForSequenceClassification LOAD REPORT from: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computing token lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7601/7601 [00:03<00:00, 2316.07it/s]\n",
      "Running VANILLA: 100%|██████████| 100/100 [00:14<00:00,  6.87it/s]\n",
      "Running PROPOSED: 100%|██████████| 100/100 [00:08<00:00, 12.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "[VANILLA] Queries: 100 | total pairs: 7,601\n",
      "[VANILLA] Per-query CE latency (feed 32 at a time):\n",
      "  mean = 144.8 ms   median = 148.4 ms   min/max = 102.0/472.8 ms\n",
      "  total wall time = 14.56 s\n",
      "\n",
      "[VANILLA] Token padding waste (per 32-sized chunk):\n",
      "  avg padded length  = 398.9 tokens\n",
      "  avg actual length  = 186.6 tokens\n",
      "  avg padding waste  = 53.2% (lower is better)\n",
      "  chunks processed   = 300\n",
      "\n",
      "========================================\n",
      "[PROPOSED] Queries: 100 | total pairs: 7,601\n",
      "[PROPOSED] Per-query CE latency (feed 32 at a time):\n",
      "  mean = 82.0 ms   median = 81.9 ms   min/max = 67.6/95.5 ms\n",
      "  total wall time = 8.23 s\n",
      "\n",
      "[PROPOSED] Token padding waste (per 32-sized chunk):\n",
      "  avg padded length  = 266.2 tokens\n",
      "  avg actual length  = 186.6 tokens\n",
      "  avg padding waste  = 29.9% (lower is better)\n",
      "  chunks processed   = 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Load Cross Encoder ---\n",
    "# Using the token fix to ensure it loads\n",
    "print(\"Loading Cross-Encoder...\")\n",
    "model = CrossEncoder(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# --- Pre-compute Token Lengths (Crucial for Proposed Method) ---\n",
    "print(\"Pre-computing token lengths...\")\n",
    "for row in tqdm(retrieved_records):\n",
    "    # Fast tokenization (no padding yet) just to get length\n",
    "    tokens = tokenizer(row['query'], row['passage'], truncation=True, max_length=512)\n",
    "    row['token_len'] = len(tokens['input_ids'])\n",
    "\n",
    "# --- Define Benchmark Function ---\n",
    "def run_benchmark(records, method_name, sort_by_length=False):\n",
    "    grouped_data = {}\n",
    "    for r in records:\n",
    "        if r['qid'] not in grouped_data: grouped_data[r['qid']] = []\n",
    "        grouped_data[r['qid']].append(r)\n",
    "        \n",
    "    query_latencies = []\n",
    "    total_padding_waste = []\n",
    "    total_actual_tokens = []\n",
    "    total_chunks = 0\n",
    "    start_global = time.time()\n",
    "    \n",
    "    for qid, docs in tqdm(grouped_data.items(), desc=f\"Running {method_name}\"):\n",
    "        t0 = time.time()\n",
    "        \n",
    "        # STRATEGY: Sort vs Shuffle\n",
    "        if sort_by_length:\n",
    "            docs.sort(key=lambda x: x['token_len']) # PROPOSED\n",
    "        else:\n",
    "            np.random.RandomState(42).shuffle(docs) # VANILLA\n",
    "            \n",
    "        pairs = [[d['query'], d['passage']] for d in docs]\n",
    "        \n",
    "        # BATCHING LOOP\n",
    "        for i in range(0, len(pairs), FEED_SIZE):\n",
    "            batch_pairs = pairs[i : i + FEED_SIZE]\n",
    "            \n",
    "            # 1. Measure Padding Waste\n",
    "            encoded = tokenizer(batch_pairs, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "            actual = torch.sum(encoded['attention_mask']).item()\n",
    "            waste = (encoded['input_ids'].shape[0] * encoded['input_ids'].shape[1]) - actual\n",
    "            \n",
    "            total_actual_tokens.append(actual)\n",
    "            total_padding_waste.append(waste)\n",
    "            total_chunks += 1\n",
    "            \n",
    "            # 2. Run Inference\n",
    "            _ = model.predict(batch_pairs, batch_size=FEED_SIZE, show_progress_bar=False)\n",
    "\n",
    "        query_latencies.append((time.time() - t0) * 1000)\n",
    "\n",
    "    total_wall_time = time.time() - start_global\n",
    "    \n",
    "    return {\n",
    "        \"latencies\": query_latencies,\n",
    "        \"wall_time\": total_wall_time,\n",
    "        \"chunks\": total_chunks,\n",
    "        \"avg_actual\": np.mean(total_actual_tokens) / FEED_SIZE,\n",
    "        \"avg_padded\": (np.sum(total_actual_tokens) + np.sum(total_padding_waste)) / total_chunks / FEED_SIZE,\n",
    "        \"waste_pct\": (np.sum(total_padding_waste) / (np.sum(total_actual_tokens) + np.sum(total_padding_waste))) * 100\n",
    "    }\n",
    "\n",
    "# --- EXECUTE BENCHMARKS ---\n",
    "\n",
    "# 1. VANILLA\n",
    "results_vanilla = run_benchmark(retrieved_records, \"VANILLA\", sort_by_length=False)\n",
    "\n",
    "# 2. PROPOSED\n",
    "results_proposed = run_benchmark(retrieved_records, \"PROPOSED\", sort_by_length=True)\n",
    "\n",
    "# --- PRINT REPORTS ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"[VANILLA] Queries: {len(queries)} | total pairs: {len(retrieved_records):,}\")\n",
    "print(f\"[VANILLA] Per-query CE latency (feed {FEED_SIZE} at a time):\")\n",
    "lats_v = results_vanilla['latencies']\n",
    "print(f\"  mean = {np.mean(lats_v):.1f} ms   median = {np.median(lats_v):.1f} ms   min/max = {np.min(lats_v):.1f}/{np.max(lats_v):.1f} ms\")\n",
    "print(f\"  total wall time = {results_vanilla['wall_time']:.2f} s\")\n",
    "print(f\"\\n[VANILLA] Token padding waste (per {FEED_SIZE}-sized chunk):\")\n",
    "print(f\"  avg padded length  = {results_vanilla['avg_padded']:.1f} tokens\")\n",
    "print(f\"  avg actual length  = {results_vanilla['avg_actual']:.1f} tokens\")\n",
    "print(f\"  avg padding waste  = {results_vanilla['waste_pct']:.1f}% (lower is better)\")\n",
    "print(f\"  chunks processed   = {results_vanilla['chunks']:,}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"[PROPOSED] Queries: {len(queries)} | total pairs: {len(retrieved_records):,}\")\n",
    "print(f\"[PROPOSED] Per-query CE latency (feed {FEED_SIZE} at a time):\")\n",
    "lats_p = results_proposed['latencies']\n",
    "print(f\"  mean = {np.mean(lats_p):.1f} ms   median = {np.median(lats_p):.1f} ms   min/max = {np.min(lats_p):.1f}/{np.max(lats_p):.1f} ms\")\n",
    "print(f\"  total wall time = {results_proposed['wall_time']:.2f} s\")\n",
    "print(f\"\\n[PROPOSED] Token padding waste (per {FEED_SIZE}-sized chunk):\")\n",
    "print(f\"  avg padded length  = {results_proposed['avg_padded']:.1f} tokens\")\n",
    "print(f\"  avg actual length  = {results_proposed['avg_actual']:.1f} tokens\")\n",
    "print(f\"  avg padding waste  = {results_proposed['waste_pct']:.1f}% (lower is better)\")\n",
    "print(f\"  chunks processed   = {results_proposed['chunks']:,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
