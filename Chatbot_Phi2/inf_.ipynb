{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74b2dcdc",
   "metadata": {},
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f917e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import re\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from peft import PeftModel\n",
    "\n",
    "# # --- SETUP ---\n",
    "# BASE_MODEL_ID = \"microsoft/phi-2\"\n",
    "# TUNED_MODEL_PATH = \"models/phi2_retail_native_bf16_c6e0c0\" # Replace with your run hash\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(TUNED_MODEL_PATH, trust_remote_code=True)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     BASE_MODEL_ID,\n",
    "#     # torch_dtype=torch.bfloat16,\n",
    "#     dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "# model = PeftModel.from_pretrained(base_model, TUNED_MODEL_PATH)\n",
    "# model.eval()\n",
    "\n",
    "\n",
    "# SYSTEM_PROMPT = (\n",
    "#     \"You are the PUMA Holographic Assistant. Follow these strict operational rules:\\n\"\n",
    "#     \"1. If Context is 'N/A': Handle general greetings or PUMA-related brand questions. \"\n",
    "#     \"If the query is completely unrelated to PUMA, sports, or retail, politely refuse to answer.\\n\"\n",
    "#     \"2. If Context is 'No products found.': Inform the user that no matching footwear was found \"\n",
    "#     \"and suggest they try a different style or category.\\n\"\n",
    "#     \"3. If Context contains Product Lists: Provide a high-level highlight of the collection \"\n",
    "#     \"and transition the user into the immersive 3D view.\\n\"\n",
    "#     \"4. If Context contains T&C/Policies: Use the information provided to answer the user query accurately.\\n\"\n",
    "#     \"5. If User Query is '<GESTURE_EXIT>': Acknowledge that the user has closed the 3D display, \"\n",
    "#     \"briefly summarize the product they just viewed, and ask if they need further assistance.\"\n",
    "# )\n",
    "\n",
    "# while True:\n",
    "#     print(\"\\n\" + \"=\"*60)\n",
    "#     context = input(\"CONTEXT: \")\n",
    "#     query = input(\"QUERY: \")\n",
    "#     if query.lower() == 'exit': break\n",
    "\n",
    "#     prompt = (\n",
    "#         f\"### Instruction:\\n{SYSTEM_PROMPT}\\n\\n\"\n",
    "#         f\"### Context:\\n{context}\\n\\n\"\n",
    "#         f\"### User Query:\\n{query}\\n\\n\"\n",
    "#         f\"### Response:\\n\"\n",
    "#     )\n",
    "\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model.generate(\n",
    "#             **inputs,\n",
    "#             max_new_tokens=256,\n",
    "#             # temperature=0.7,\n",
    "#             do_sample=False,\n",
    "#             pad_token_id=tokenizer.eos_token_id,\n",
    "#             # We DON'T set eos_token_id here so we can see if the model \n",
    "#             # stops naturally or keeps rambling\n",
    "#         )\n",
    "\n",
    "#     # 1. Decode EVERYTHING (Prompt + Generated Text)\n",
    "#     # We set clean_up_tokenization_spaces to False to see the raw output\n",
    "#     raw_output = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "#     # print(\"\\n--- [DEBUG: FULL RAW MODEL OUTPUT] ---\")\n",
    "#     # print(raw_output)\n",
    "#     # print(\"--- [END DEBUG] ---\\n\")\n",
    "#     # Extract assistant response only\n",
    "#     response_text = raw_output.split(\"### Response:\\n\", 1)[-1]\n",
    "\n",
    "#     # Remove END_OF_RESPONSE or partial tokens\n",
    "#     response_text = re.sub(r\"<END_OF_RESPONSE.*\", \"\", response_text, flags=re.DOTALL)\n",
    "\n",
    "#     # Cleanup\n",
    "#     response_text = response_text.strip()\n",
    "\n",
    "#     print(\"\\n--- [CLEAN MODEL RESPONSE] ---\")\n",
    "#     print(response_text)\n",
    "#     print(\"--- [END RESPONSE] ---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec09688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteria, StoppingCriteriaList\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    \"\"\"Stop generation when a specific token sequence appears at the end.\"\"\"\n",
    "    def __init__(self, stop_ids: list[int]):\n",
    "        if not stop_ids:\n",
    "            raise ValueError(\"stop_ids is empty\")\n",
    "        self.stop_ids = stop_ids\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        # input_ids: [batch, seq_len]\n",
    "        if input_ids.shape[0] != 1:\n",
    "            # your use case is 1 anyway\n",
    "            return False\n",
    "        n = len(self.stop_ids)\n",
    "        if input_ids.shape[1] < n:\n",
    "            return False\n",
    "        tail = input_ids[0, -n:].tolist()\n",
    "        return tail == self.stop_ids\n",
    "\n",
    "\n",
    "# --- SETUP ---\n",
    "BASE_MODEL_ID = \"microsoft/phi-2\"\n",
    "# TUNED_MODEL_PATH = \"models/phi2_retail_native_bf16_c6e0c0\"\n",
    "TUNED_MODEL_PATH = \"models/phi2_retail_native_bf16_38f4a5\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TUNED_MODEL_PATH, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, TUNED_MODEL_PATH)\n",
    "model.eval()\n",
    "\n",
    "# IMPORTANT: stop as soon as the model emits the first end tag\n",
    "# Use the full \"<END_OF_RESPONSE>\" token sequence (most robust)\n",
    "stop_str = \"<END_OF_RESPONSE>\"\n",
    "stop_ids = tokenizer.encode(stop_str, add_special_tokens=False)\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens(stop_ids)])\n",
    "\n",
    "# SYSTEM_PROMPT = (\n",
    "#     \"You are the PUMA Holographic Assistant. Follow these strict operational rules:\\n\"\n",
    "#     \"1. If Context is 'N/A': Handle general greetings or PUMA-related brand questions. \"\n",
    "#     \"If the query is completely unrelated to PUMA, sports, or retail, politely refuse to answer.\\n\"\n",
    "#     \"2. If Context is 'No products found.': Inform the user that no matching footwear was found \"\n",
    "#     \"and suggest they try a different style or category.\\n\"\n",
    "#     \"3. If Context contains Product Lists: Provide a high-level highlight of the collection \"\n",
    "#     \"and transition the user into the immersive 3D view.\\n\"\n",
    "#     \"4. If Context contains T&C/Policies: Use the information provided to answer the user query accurately.\\n\"\n",
    "#     \"5. If User Query is '<GESTURE_EXIT>': Acknowledge that the user has closed the 3D display, \"\n",
    "#     \"briefly summarize the product they just viewed, and ask if they need further assistance.\\n\\n\"\n",
    "# )\n",
    "\n",
    "### Version 2\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are the PUMA Holographic Assistant, an intelligent 3D AI retail guide. \"\n",
    "    \"Follow these strict operational rules:\\n\"\n",
    "    \"1. If Context is 'N/A' and the user greets you, says goodbye, or asks who you are: \"\n",
    "    \"Respond enthusiastically in character as the PUMA Holographic AI Assistant and pivot to exploring PUMA gear.\\n\"\n",
    "    \"2. If Context is 'N/A' and the query is completely unrelated to PUMA, sports, or retail: \"\n",
    "    \"Politely refuse to answer, stay in character, and pivot back to PUMA footwear or gear.\\n\"\n",
    "    \"3. If Context is 'No products found.': Inform the user that no matching footwear was found \"\n",
    "    \"and suggest they try a different style or category.\\n\"\n",
    "    \"4. If Context contains Product Lists: Provide a high-level highlight of the collection \"\n",
    "    \"and transition the user into the immersive 3D view.\\n\"\n",
    "    \"5. If Context contains T&C/Policies: Use the information provided to answer the user query accurately.\\n\"\n",
    "    \"6. If User Query is '<GESTURE_EXIT>': Acknowledge that the user has closed the 3D display, \"\n",
    "    \"briefly summarize the product they just viewed, and ask if they need further assistance.\"\n",
    ")\n",
    "\n",
    "while True:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    context = input(\"CONTEXT: \")\n",
    "    query = input(\"QUERY: \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    prompt = (\n",
    "        f\"### Instruction:\\n{SYSTEM_PROMPT}\\n\\n\"\n",
    "        f\"### Context:\\n{context}\\n\\n\"\n",
    "        f\"### User Query:\\n{query}\\n\\n\"\n",
    "        f\"### Response:\\n\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    print(\"\\n[DEBUG] Context:\", context)\n",
    "    print(\"[DEBUG] Query:\", query)\n",
    "\n",
    "    prompt_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,  # safe ceiling; should stop much earlier now\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            repetition_penalty=1.05,  # mild anti-looping\n",
    "        )\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    new_tokens = outputs.shape[-1] - prompt_len\n",
    "    infer_time = t1 - t0\n",
    "    print(f\"\\n[Inference Time] {infer_time:.3f}s | prompt_tokens: {prompt_len} | new_tokens: {new_tokens} | tok/s: {new_tokens / max(infer_time, 1e-9):.2f}\")\n",
    "\n",
    "    raw_output = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "    response_text = raw_output.split(\"### Response:\\n\", 1)[-1]\n",
    "    # Cut at first end tag (fast + clean)\n",
    "    response_text = response_text.split(\"<END_OF_RESPONSE>\", 1)[0]\n",
    "    # Keep your regex as extra safety\n",
    "    response_text = re.sub(r\"<END_OF_RESPONSE.*\", \"\", response_text, flags=re.DOTALL).strip()\n",
    "\n",
    "    print(\"\\n--- [CLEAN MODEL RESPONSE] ---\")\n",
    "    print(response_text)\n",
    "    print(\"--- [END RESPONSE] ---\\n\")\n",
    "\n",
    "    print(\"\\n--- [RAW MODEL RESPONSE] ---\")\n",
    "    print(raw_output)\n",
    "    print(\"--- [END RESPONSE] ---\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
