{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "590eaa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wwon0076\\Desktop\\FYP\\Chatbot_Phi2\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import uuid\n",
    "from datasets import Dataset\n",
    "from transformers import default_data_collator\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorForLanguageModeling,\n",
    "    set_seed\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7274c987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version: 2.4.1+cu121\n",
      "CUDA Available: True\n",
      "GPU Name: NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "#  Check Versions \n",
    "print(f\"Torch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU NOT DETECTED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d857880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reproducibility locked with Seed: 42\n"
     ]
    }
   ],
   "source": [
    "#  REPRODUCIBILITY SETUP \n",
    "SEED = 42\n",
    "\n",
    "def set_reproducibility(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    set_seed(seed)\n",
    "    print(f\"Reproducibility locked with Seed: {seed}\")\n",
    "\n",
    "set_reproducibility(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15785419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Folder: models/phi2_retail_native_bf16_38f4a5\n"
     ]
    }
   ],
   "source": [
    "#  PATHS \n",
    "TARGET_FILE = \"data/puma_clean_dataset_smart_v2.json\"          \n",
    "MODEL_PATH = \"microsoft/phi-2\"   \n",
    "\n",
    "# Unique run hash\n",
    "run_hash = str(uuid.uuid4())[:6]\n",
    "TUNED_MODEL_PATH = f\"models/phi2_retail_native_bf16_{run_hash}\"\n",
    "\n",
    "print(f\"Output Folder: {TUNED_MODEL_PATH}\")\n",
    "\n",
    "#  HYPERPARAMETERS \n",
    "MAX_LENGTH = 1024       \n",
    "RANK = 32               \n",
    "ALPHA = 64             \n",
    "DROPOUT = 0.05\n",
    "LEARNING_RATE = 2e-4\n",
    "BATCH_SIZE = 6          \n",
    "GRAD_ACC_STEPS = 4      \n",
    "NUM_EPOCHS = 10         \n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "#  THE ANCHOR (Updated with ASIN Rule) \n",
    "# SYSTEM_PROMPT = (\n",
    "#     \"You are the PUMA Holographic Assistant. Follow these strict operational rules:\\n\"\n",
    "#     \"1. If Context is 'N/A': Handle general greetings or PUMA-related brand questions. \"\n",
    "#     \"If the query is completely unrelated to PUMA, sports, or retail, politely refuse to answer.\\n\"\n",
    "#     \"2. If Context is 'No products found.': Inform the user that no matching footwear was found \"\n",
    "#     \"and suggest they try a different style or category.\\n\"\n",
    "#     \"3. If Context contains Product Lists: Provide a high-level highlight of the collection \"\n",
    "#     \"and transition the user into the immersive 3D view.\\n\"\n",
    "#     \"4. If Context contains T&C/Policies: Use the information provided to answer the user query accurately.\\n\"\n",
    "#     \"5. If User Query is '<GESTURE_EXIT>': Acknowledge that the user has closed the 3D display, \"\n",
    "#     \"briefly summarize the product they just viewed, and ask if they need further assistance.\"\n",
    "# )\n",
    "\n",
    "## version 2 (so OOD dont get easily triggered)\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are the PUMA Holographic Assistant, an intelligent 3D AI retail guide. \"\n",
    "    \"Follow these strict operational rules:\\n\"\n",
    "    \"1. If Context is 'N/A' and the user greets you, says goodbye, or asks who you are: \"\n",
    "    \"Respond enthusiastically in character as the PUMA Holographic AI Assistant and pivot to exploring PUMA gear.\\n\"\n",
    "    \"2. If Context is 'N/A' and the query is completely unrelated to PUMA, sports, or retail: \"\n",
    "    \"Politely refuse to answer, stay in character, and pivot back to PUMA footwear or gear.\\n\"\n",
    "    \"3. If Context is 'No products found.': Inform the user that no matching footwear was found \"\n",
    "    \"and suggest they try a different style or category.\\n\"\n",
    "    \"4. If Context contains Product Lists: Provide a high-level highlight of the collection \"\n",
    "    \"and transition the user into the immersive 3D view.\\n\"\n",
    "    \"5. If Context contains T&C/Policies: Use the information provided to answer the user query accurately.\\n\"\n",
    "    \"6. If User Query is '<GESTURE_EXIT>': Acknowledge that the user has closed the 3D display, \"\n",
    "    \"briefly summarize the product they just viewed, and ask if they need further assistance.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7eed1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows Loaded: 1649\n",
      "\n",
      "SAMPLE RAW ENTRY (Index 0)\n",
      "{'context': 'For your convenience, we accept payment by credit and debit cards, Apple Pay, PayPal, Afterpay, Google Pay and PUMA Gift Cards. Paying by Credit/Debit Card We accept the following cards: Visa Mastercard American Express Discover For your security, we will contact your card issuer to request pre-authorization for your order amount. Your card will not be charged until after your order has shipped. Paying with Afterpay Shop now and pay later with Afterpay, now available on us. puma.', 'user': 'What payment options can I use at PUMA, and how does the creditcard preauthorization work?', 'assistant': \"We're thrilled you asked  you can zip through checkout with Visa, Mastercard, American Express, Discover, Apple Pay, Google Pay, PayPal, Afterpay or a PUMA Gift Card! When you choose a credit or debit card, we reach out to your issuer for a quick preauthorization, but don't worry, your card isn't charged until your order ships. Happy shopping!\"}\n"
     ]
    }
   ],
   "source": [
    "# Load all fine tuning datasets\n",
    "all_data = []\n",
    "\n",
    "with open(TARGET_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    entries = json.load(f)\n",
    "\n",
    "# Normalize into list\n",
    "if isinstance(entries, list):\n",
    "    all_data = entries\n",
    "else:\n",
    "    all_data = [entries]\n",
    "\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "print(f\"Total Rows Loaded: {len(df)}\")\n",
    "print(\"\\nSAMPLE RAW ENTRY (Index 0)\")\n",
    "print(df.iloc[0].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3de8638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Found layer: model.layers.0.self_attn.q_proj\n",
      "   Found layer: model.layers.0.self_attn.k_proj\n",
      "   Found layer: model.layers.0.self_attn.v_proj\n",
      "   Found layer: model.layers.0.mlp.fc1\n",
      "   Found layer: model.layers.0.mlp.fc2\n"
     ]
    }
   ],
   "source": [
    "# Load phi 2 model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH, \n",
    "    trust_remote_code=True, \n",
    "    torch_dtype=torch.bfloat16,  \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Enable memory saving\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "# check first block to see if 'q_proj', 'fc1' etc exist\n",
    "for name, module in model.named_modules():\n",
    "    if \"layers.0\" in name and \"proj\" in name: \n",
    "        print(f\"   Found layer: {name}\")\n",
    "    if \"layers.0\" in name and \"fc\" in name:\n",
    "        print(f\"   Found layer: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c4c9773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LoRA ADAPTER ATTACHED:\n",
      "trainable params: 47,185,920 || all params: 2,826,869,760 || trainable%: 1.6692\n"
     ]
    }
   ],
   "source": [
    "# Based on inspection, these are the standard Phi-2 modules\n",
    "targets = [\"q_proj\", \"k_proj\", \"v_proj\", \"fc1\", \"fc2\", \"dense\"]\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=RANK,\n",
    "    lora_alpha=ALPHA,\n",
    "    target_modules=targets,\n",
    "    lora_dropout=DROPOUT,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "print(\"\\nLoRA ADAPTER ATTACHED:\")\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28a31457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1649/1649 [00:05<00:00, 300.37 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DECODED TRAINING SAMPLE (WHAT THE MODEL SEES) \n",
      "### Instruction:\n",
      "You are the PUMA Holographic Assistant, an intelligent 3D AI retail guide. Follow these strict operational rules:\n",
      "1. If Context is 'N/A' and the user greets you, says goodbye, or asks who you are: Respond enthusiastically in character as the PUMA Holographic AI Assistant and pivot to exploring PUMA gear.\n",
      "2. If Context is 'N/A' and the query is completely unrelated to PUMA, sports, or retail: Politely refuse to answer, stay in character, and pivot back to PUMA footwear or gear.\n",
      "3. If Context is 'No products found.': Inform the user that no matching footwear was found and suggest they try a different style or category.\n",
      "4. If Context contains Product Lists: Provide a high-level highlight of the collection and transition the user into the immersive 3D view.\n",
      "5. If Context contains T&C/Policies: Use the information provided to answer the user query accurately.\n",
      "6. If User Query is '<GESTURE_EXIT>': Acknowledge that the user has closed the 3D display, briefly summarize the product they just viewed, and ask if they need further assistance.\n",
      "\n",
      "### Context:\n",
      "For your convenience, we accept payment by credit and debit cards, Apple Pay, PayPal, Afterpay, Google Pay and PUMA Gift Cards. Paying by Credit/Debit Card We accept the following cards: Visa Mastercard American Express Discover For your security, we will contact your card issuer to request pre-authorization for your order amount. Your card will not be charged until after your order has shipped. Paying with Afterpay Shop now and pay later with Afterpay, now available on us. puma.\n",
      "\n",
      "### User Query:\n",
      "What payment options do you accept and how does Afterpay work?\n",
      "\n",
      "### Response:\n",
      "Great news! We take Visa, Mastercard, American Express, Discover, Apple Pay, Google Pay, PayPal, PUMA Gift Cards, and the superconvenient Afterpayshop now and pay later with no interest, and we'll only charge your card once your order ships.\n",
      "<END_OF_RESPONSE>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" \n",
    "\n",
    "def format_and_tokenize(row):\n",
    "\n",
    "    raw_context = row['context']\n",
    "    user_query = row['user']\n",
    "    assistant_response = row['assistant']\n",
    "\n",
    "    # Explicit stop marker\n",
    "    END_TOKEN = \"<END_OF_RESPONSE>\"\n",
    "\n",
    "    full_text = (\n",
    "        f\"### Instruction:\\n{SYSTEM_PROMPT}\\n\\n\"\n",
    "        f\"### Context:\\n{raw_context}\\n\\n\"\n",
    "        f\"### User Query:\\n{user_query}\\n\\n\"\n",
    "        f\"### Response:\\n\"\n",
    "        f\"{assistant_response}\\n{END_TOKEN}{tokenizer.eos_token}\"\n",
    "    )\n",
    "\n",
    "    user_part = (\n",
    "        f\"### Instruction:\\n{SYSTEM_PROMPT}\\n\\n\"\n",
    "        f\"### Context:\\n{raw_context}\\n\\n\"\n",
    "        f\"### User Query:\\n{user_query}\\n\\n\"\n",
    "        f\"### Response:\\n\"\n",
    "    )\n",
    "\n",
    "    tokenized_full = tokenizer(\n",
    "        full_text,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    tokenized_user = tokenizer(\n",
    "        user_part,\n",
    "        add_special_tokens=False,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    input_ids = tokenized_full[\"input_ids\"]\n",
    "    labels = list(input_ids)\n",
    "\n",
    "    # Mask everything before assistant response\n",
    "    user_len = len(tokenized_user[\"input_ids\"])\n",
    "    for i in range(min(user_len, len(labels))):\n",
    "        labels[i] = -100\n",
    "\n",
    "    # Mask padding\n",
    "    for i, token_id in enumerate(input_ids):\n",
    "        if token_id == tokenizer.pad_token_id:\n",
    "            labels[i] = -100\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids, ## this is the full text in tokenized format ([ 50,  60,  99,   0,   0,   0,   0,   0 ])\n",
    "        \"attention_mask\": tokenized_full[\"attention_mask\"], ## this is the mask we will be mask ([  1,   1,   1,   0,   0,   0,   0,   0 ])\n",
    "        \"labels\": labels ## this is the labels where grading will happened ([-100,  60,  99,-100,-100,-100,-100,-100 ])\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply formatting\n",
    "dataset = Dataset.from_pandas(df)\n",
    "tokenized_dataset = dataset.map(format_and_tokenize, remove_columns=list(df.columns))\n",
    "\n",
    "# \n",
    "print(\"\\nDECODED TRAINING SAMPLE (WHAT THE MODEL SEES) \")\n",
    "decoded_sample = tokenizer.decode(tokenized_dataset[1]['input_ids'], skip_special_tokens=True)\n",
    "print(decoded_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ae39a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# # -- ANALYSIS --\n",
    "# lengths = []\n",
    "# assistant_lengths = []\n",
    "# truncated = 0\n",
    "\n",
    "# for row in tqdm(all_data):\n",
    "#     full_text = (\n",
    "#         f\"### Instruction:\\n{SYSTEM_PROMPT}\\n\\n\"\n",
    "#         f\"### Context:\\n{row['context']}\\n\\n\"\n",
    "#         f\"### User Query:\\n{row['user']}\\n\\n\"\n",
    "#         f\"### Response:\\n{row['assistant']}{tokenizer.eos_token}\"\n",
    "#     )\n",
    "\n",
    "#     user_part = (\n",
    "#         f\"### Instruction:\\n{SYSTEM_PROMPT}\\n\\n\"\n",
    "#         f\"### Context:\\n{row['context']}\\n\\n\"\n",
    "#         f\"### User Query:\\n{row['user']}\\n\\n\"\n",
    "#         f\"### Response:\\n\"\n",
    "#     )\n",
    "\n",
    "#     full_ids = tokenizer(full_text, add_special_tokens=False)[\"input_ids\"]\n",
    "#     user_ids = tokenizer(user_part, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "#     total_len = len(full_ids)\n",
    "#     assistant_len = max(0, total_len - len(user_ids))\n",
    "\n",
    "#     lengths.append(total_len)\n",
    "#     assistant_lengths.append(assistant_len)\n",
    "\n",
    "#     if total_len > MAX_LENGTH:\n",
    "#         truncated += 1\n",
    "\n",
    "# # -- REPORT --\n",
    "# lengths = np.array(lengths)\n",
    "# assistant_lengths = np.array(assistant_lengths)\n",
    "\n",
    "# print(\"\\n====== TOKEN LENGTH REPORT ======\")\n",
    "# print(f\"Max tokens           : {lengths.max()}\")\n",
    "# print(f\"Mean tokens          : {lengths.mean():.1f}\")\n",
    "# print(f\"95th percentile      : {np.percentile(lengths, 95):.1f}\")\n",
    "# print(f\"Samples > {MAX_LENGTH}: {truncated} ({truncated / len(lengths) * 100:.2f}%)\")\n",
    "\n",
    "# print(\"\\n====== ASSISTANT RESPONSE LENGTH ======\")\n",
    "# print(f\"Max assistant tokens : {assistant_lengths.max()}\")\n",
    "# print(f\"Mean assistant tokens: {assistant_lengths.mean():.1f}\")\n",
    "\n",
    "# print(\"\\n====== RECOMMENDATION ======\")\n",
    "# if np.percentile(lengths, 95) < 512:\n",
    "#     print(\" 1024 is excessive — reduce MAX_LENGTH to 512\")\n",
    "# elif np.percentile(lengths, 95) < 768:\n",
    "#     print(\"1024 is mostly unused — consider 768\")\n",
    "# else:\n",
    "#     print(\" 1024 is justified for your dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "596e6dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: 1484 rows\n",
      "Test Set:     165 rows\n"
     ]
    }
   ],
   "source": [
    "# Shuffle and split with fixed seed\n",
    "tokenized_dataset = tokenized_dataset.shuffle(seed=SEED)\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=SEED)\n",
    "\n",
    "print(f\"Training Set: {len(split_dataset['train'])} rows\")\n",
    "print(f\"Test Set:     {len(split_dataset['test'])} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d0f739b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wwon0076\\AppData\\Local\\Temp\\ipykernel_46968\\1707233322.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer Initialized.\n"
     ]
    }
   ],
   "source": [
    "# Fine tuning setup\n",
    "data_collator = default_data_collator\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=TUNED_MODEL_PATH,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=5,\n",
    "    seed=SEED,                  \n",
    "    data_seed=SEED,             \n",
    "    \n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    \n",
    "    fp16=False,\n",
    "    bf16=True,             \n",
    "    \n",
    "    warmup_steps=20,\n",
    "    save_total_limit=2,\n",
    "    # group_by_length=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model, ## LoRA adapter config.\n",
    "    args=training_args, ## Fine Tuning config.\n",
    "    train_dataset=split_dataset['train'],\n",
    "    eval_dataset=split_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "print(\"Trainer Initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f2c9e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+++++++++++++ STARTING TRAINING +++++++++++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "c:\\Users\\wwon0076\\Desktop\\FYP\\Chatbot_Phi2\\venv\\lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:96: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "c:\\Users\\wwon0076\\Desktop\\FYP\\Chatbot_Phi2\\venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 2:48:17, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.516600</td>\n",
       "      <td>1.433009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.346200</td>\n",
       "      <td>1.243407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.195200</td>\n",
       "      <td>1.158260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.085400</td>\n",
       "      <td>1.135347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.081400</td>\n",
       "      <td>1.117189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.019400</td>\n",
       "      <td>1.090308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.962700</td>\n",
       "      <td>1.081613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.913200</td>\n",
       "      <td>1.071211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.917900</td>\n",
       "      <td>1.063907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.901900</td>\n",
       "      <td>1.057081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.857800</td>\n",
       "      <td>1.046688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.862100</td>\n",
       "      <td>1.046976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wwon0076\\Desktop\\FYP\\Chatbot_Phi2\\venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "c:\\Users\\wwon0076\\Desktop\\FYP\\Chatbot_Phi2\\venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "c:\\Users\\wwon0076\\Desktop\\FYP\\Chatbot_Phi2\\venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "c:\\Users\\wwon0076\\Desktop\\FYP\\Chatbot_Phi2\\venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "c:\\Users\\wwon0076\\Desktop\\FYP\\Chatbot_Phi2\\venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "c:\\Users\\wwon0076\\Desktop\\FYP\\Chatbot_Phi2\\venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "c:\\Users\\wwon0076\\Desktop\\FYP\\Chatbot_Phi2\\venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "c:\\Users\\wwon0076\\Desktop\\FYP\\Chatbot_Phi2\\venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "c:\\Users\\wwon0076\\Desktop\\FYP\\Chatbot_Phi2\\venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "c:\\Users\\wwon0076\\Desktop\\FYP\\Chatbot_Phi2\\venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "c:\\Users\\wwon0076\\Desktop\\FYP\\Chatbot_Phi2\\venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "c:\\Users\\wwon0076\\Desktop\\FYP\\Chatbot_Phi2\\venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+++++++++++++ SAVING MODEL +++++++++++++\n",
      "DONE! Model saved to: models/phi2_retail_native_bf16_38f4a5\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n+++++++++++++ STARTING TRAINING +++++++++++++\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n+++++++++++++ SAVING MODEL +++++++++++++\")\n",
    "trainer.model.save_pretrained(TUNED_MODEL_PATH)\n",
    "tokenizer.save_pretrained(TUNED_MODEL_PATH)\n",
    "print(f\"DONE! Model saved to: {TUNED_MODEL_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
