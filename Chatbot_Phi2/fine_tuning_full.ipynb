{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590eaa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import uuid\n",
    "from datasets import Dataset\n",
    "from transformers import default_data_collator\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorForLanguageModeling,\n",
    "    set_seed\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7274c987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Check Versions \n",
    "print(f\"Torch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU NOT DETECTED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d857880",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  REPRODUCIBILITY SETUP \n",
    "SEED = 42\n",
    "\n",
    "def set_reproducibility(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    set_seed(seed)\n",
    "    print(f\"Reproducibility locked with Seed: {seed}\")\n",
    "\n",
    "set_reproducibility(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15785419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  PATHS \n",
    "INPUT_JSON_DIR = \"data\"          \n",
    "MODEL_PATH = \"microsoft/phi-2\"   \n",
    "\n",
    "# Unique run hash\n",
    "run_hash = str(uuid.uuid4())[:6]\n",
    "TUNED_MODEL_PATH = f\"models/phi2_retail_native_bf16_{run_hash}\"\n",
    "\n",
    "print(f\"Output Folder: {TUNED_MODEL_PATH}\")\n",
    "\n",
    "#  HYPERPARAMETERS \n",
    "MAX_LENGTH = 1024       \n",
    "RANK = 32               \n",
    "ALPHA = 64             \n",
    "DROPOUT = 0.05\n",
    "LEARNING_RATE = 2e-4\n",
    "BATCH_SIZE = 6          \n",
    "GRAD_ACC_STEPS = 4      \n",
    "NUM_EPOCHS = 10         \n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "#  THE ANCHOR (Updated with ASIN Rule) \n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are the PUMA Holographic Assistant. Follow these strict operational rules:\\n\"\n",
    "    \"1. If Context is 'N/A': Handle general greetings or PUMA-related brand questions. \"\n",
    "    \"If the query is completely unrelated to PUMA, sports, or retail, politely refuse to answer.\\n\"\n",
    "    \"2. If Context is 'No products found.': Inform the user that no matching footwear was found \"\n",
    "    \"and suggest they try a different style or category.\\n\"\n",
    "    \"3. If Context contains Product Lists: Provide a high-level highlight of the collection \"\n",
    "    \"and transition the user into the immersive 3D view.\\n\"\n",
    "    \"4. If Context contains T&C/Policies: Use the information provided to answer the user query accurately.\\n\"\n",
    "    \"5. If User Query is '<GESTURE_EXIT>': Acknowledge that the user has closed the 3D display, \"\n",
    "    \"briefly summarize the product they just viewed, and ask if they need further assistance.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eed1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all fine tuning datasets\n",
    "all_data = []\n",
    "if os.path.isdir(INPUT_JSON_DIR):\n",
    "    for filename in os.listdir(INPUT_JSON_DIR):\n",
    "        if filename.endswith(\".json\"):\n",
    "            filepath = os.path.join(INPUT_JSON_DIR, filename)\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                entries = json.load(f)\n",
    "                if isinstance(entries, list):\n",
    "                    all_data.extend(entries)\n",
    "                else:\n",
    "                    all_data.append(entries)\n",
    "\n",
    "df = pd.DataFrame(all_data)\n",
    "print(f\"Total Rows Loaded: {len(df)}\")\n",
    "\n",
    "print(\"\\n SAMPLE RAW ENTRY (Index 0) \")\n",
    "print(df.iloc[0].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load phi 2 model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH, \n",
    "    trust_remote_code=True, \n",
    "    torch_dtype=torch.bfloat16,  \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Enable memory saving\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "# check first block to see if 'q_proj', 'fc1' etc exist\n",
    "for name, module in model.named_modules():\n",
    "    if \"layers.0\" in name and \"proj\" in name: \n",
    "        print(f\"   Found layer: {name}\")\n",
    "    if \"layers.0\" in name and \"fc\" in name:\n",
    "        print(f\"   Found layer: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4c9773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on inspection, these are the standard Phi-2 modules\n",
    "targets = [\"q_proj\", \"k_proj\", \"v_proj\", \"fc1\", \"fc2\", \"dense\"]\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=RANK,\n",
    "    lora_alpha=ALPHA,\n",
    "    target_modules=targets,\n",
    "    lora_dropout=DROPOUT,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "print(\"\\nLoRA ADAPTER ATTACHED:\")\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a31457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" \n",
    "\n",
    "def format_and_tokenize(row):\n",
    "\n",
    "    raw_context = row['context']\n",
    "    user_query = row['user']\n",
    "    assistant_response = row['assistant']\n",
    "\n",
    "    # Explicit stop marker\n",
    "    END_TOKEN = \"<END_OF_RESPONSE>\"\n",
    "\n",
    "    full_text = (\n",
    "        f\"### Instruction:\\n{SYSTEM_PROMPT}\\n\\n\"\n",
    "        f\"### Context:\\n{raw_context}\\n\\n\"\n",
    "        f\"### User Query:\\n{user_query}\\n\\n\"\n",
    "        f\"### Response:\\n\"\n",
    "        f\"{assistant_response}\\n{END_TOKEN}{tokenizer.eos_token}\"\n",
    "    )\n",
    "\n",
    "    user_part = (\n",
    "        f\"### Instruction:\\n{SYSTEM_PROMPT}\\n\\n\"\n",
    "        f\"### Context:\\n{raw_context}\\n\\n\"\n",
    "        f\"### User Query:\\n{user_query}\\n\\n\"\n",
    "        f\"### Response:\\n\"\n",
    "    )\n",
    "\n",
    "    tokenized_full = tokenizer(\n",
    "        full_text,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    tokenized_user = tokenizer(\n",
    "        user_part,\n",
    "        add_special_tokens=False,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    input_ids = tokenized_full[\"input_ids\"]\n",
    "    labels = list(input_ids)\n",
    "\n",
    "    # Mask everything before assistant response\n",
    "    user_len = len(tokenized_user[\"input_ids\"])\n",
    "    for i in range(min(user_len, len(labels))):\n",
    "        labels[i] = -100\n",
    "\n",
    "    # Mask padding\n",
    "    for i, token_id in enumerate(input_ids):\n",
    "        if token_id == tokenizer.pad_token_id:\n",
    "            labels[i] = -100\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids, ## this is the full text in tokenized format ([ 50,  60,  99,   0,   0,   0,   0,   0 ])\n",
    "        \"attention_mask\": tokenized_full[\"attention_mask\"], ## this is the mask we will be mask ([  1,   1,   1,   0,   0,   0,   0,   0 ])\n",
    "        \"labels\": labels ## this is the labels where grading will happened ([-100,  60,  99,-100,-100,-100,-100,-100 ])\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply formatting\n",
    "dataset = Dataset.from_pandas(df)\n",
    "tokenized_dataset = dataset.map(format_and_tokenize, remove_columns=list(df.columns))\n",
    "\n",
    "# \n",
    "print(\"\\nDECODED TRAINING SAMPLE (WHAT THE MODEL SEES) \")\n",
    "decoded_sample = tokenizer.decode(tokenized_dataset[1]['input_ids'], skip_special_tokens=True)\n",
    "print(decoded_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ae39a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# # -- ANALYSIS --\n",
    "# lengths = []\n",
    "# assistant_lengths = []\n",
    "# truncated = 0\n",
    "\n",
    "# for row in tqdm(all_data):\n",
    "#     full_text = (\n",
    "#         f\"### Instruction:\\n{SYSTEM_PROMPT}\\n\\n\"\n",
    "#         f\"### Context:\\n{row['context']}\\n\\n\"\n",
    "#         f\"### User Query:\\n{row['user']}\\n\\n\"\n",
    "#         f\"### Response:\\n{row['assistant']}{tokenizer.eos_token}\"\n",
    "#     )\n",
    "\n",
    "#     user_part = (\n",
    "#         f\"### Instruction:\\n{SYSTEM_PROMPT}\\n\\n\"\n",
    "#         f\"### Context:\\n{row['context']}\\n\\n\"\n",
    "#         f\"### User Query:\\n{row['user']}\\n\\n\"\n",
    "#         f\"### Response:\\n\"\n",
    "#     )\n",
    "\n",
    "#     full_ids = tokenizer(full_text, add_special_tokens=False)[\"input_ids\"]\n",
    "#     user_ids = tokenizer(user_part, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "#     total_len = len(full_ids)\n",
    "#     assistant_len = max(0, total_len - len(user_ids))\n",
    "\n",
    "#     lengths.append(total_len)\n",
    "#     assistant_lengths.append(assistant_len)\n",
    "\n",
    "#     if total_len > MAX_LENGTH:\n",
    "#         truncated += 1\n",
    "\n",
    "# # -- REPORT --\n",
    "# lengths = np.array(lengths)\n",
    "# assistant_lengths = np.array(assistant_lengths)\n",
    "\n",
    "# print(\"\\n====== TOKEN LENGTH REPORT ======\")\n",
    "# print(f\"Max tokens           : {lengths.max()}\")\n",
    "# print(f\"Mean tokens          : {lengths.mean():.1f}\")\n",
    "# print(f\"95th percentile      : {np.percentile(lengths, 95):.1f}\")\n",
    "# print(f\"Samples > {MAX_LENGTH}: {truncated} ({truncated / len(lengths) * 100:.2f}%)\")\n",
    "\n",
    "# print(\"\\n====== ASSISTANT RESPONSE LENGTH ======\")\n",
    "# print(f\"Max assistant tokens : {assistant_lengths.max()}\")\n",
    "# print(f\"Mean assistant tokens: {assistant_lengths.mean():.1f}\")\n",
    "\n",
    "# print(\"\\n====== RECOMMENDATION ======\")\n",
    "# if np.percentile(lengths, 95) < 512:\n",
    "#     print(\" 1024 is excessive — reduce MAX_LENGTH to 512\")\n",
    "# elif np.percentile(lengths, 95) < 768:\n",
    "#     print(\"1024 is mostly unused — consider 768\")\n",
    "# else:\n",
    "#     print(\" 1024 is justified for your dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596e6dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and split with fixed seed\n",
    "tokenized_dataset = tokenized_dataset.shuffle(seed=SEED)\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=SEED)\n",
    "\n",
    "print(f\"Training Set: {len(split_dataset['train'])} rows\")\n",
    "print(f\"Test Set:     {len(split_dataset['test'])} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0f739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tuning setup\n",
    "data_collator = default_data_collator\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=TUNED_MODEL_PATH,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=5,\n",
    "    seed=SEED,                  \n",
    "    data_seed=SEED,             \n",
    "    \n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    \n",
    "    fp16=False,\n",
    "    bf16=True,             \n",
    "    \n",
    "    warmup_steps=20,\n",
    "    save_total_limit=2,\n",
    "    # group_by_length=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model, ## LoRA adapter config.\n",
    "    args=training_args, ## Fine Tuning config.\n",
    "    train_dataset=split_dataset['train'],\n",
    "    eval_dataset=split_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "print(\"Trainer Initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2c9e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n+++++++++++++ STARTING TRAINING +++++++++++++\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n+++++++++++++ SAVING MODEL +++++++++++++\")\n",
    "trainer.model.save_pretrained(TUNED_MODEL_PATH)\n",
    "tokenizer.save_pretrained(TUNED_MODEL_PATH)\n",
    "print(f\"DONE! Model saved to: {TUNED_MODEL_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
